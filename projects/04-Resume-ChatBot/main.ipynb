{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Q&A Chatbot 🗒️🤖\n",
    "\n",
    "The goal of this notebook is to create a command line chat interface such that if you give it the path of your resume, you should be able to ask questions about your resume by chatting with the chatbot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the Required Packages and load the ENV Vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "load_dotenv(\n",
    "    \"/Users/aryankhurana/Developer/GenAI-with-Langchain/projects/04-Resume-ChatBot/.env\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the Embedding Model and the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use the path of the resume to create a vector database\n",
    "\n",
    "#### Load the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "resume_file_path = \"/Users/aryankhurana/Developer/GenAI-with-Langchain/projects/04-Resume-ChatBot/test.pdf\"\n",
    "\n",
    "loader = None\n",
    "if resume_file_path.endswith(\".pdf\"):\n",
    "    loader = PyPDFLoader(resume_file_path)\n",
    "else:\n",
    "    loader = TextLoader(resume_file_path)\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the documents into chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=99999999999, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "\n",
    "vector_db = Qdrant.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"resume_collection\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 04: Create a function to create a chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(vector_db: Qdrant):\n",
    "    retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    template = \"\"\"\n",
    "    You are a helpful assistant that ONLY answers questions about the resume information provided below.\n",
    "    \n",
    "    RULES:\n",
    "    1. Only answer questions that are directly related to the resume information\n",
    "    2. If the question is not about the resume or the person's professional background, politely decline to answer\n",
    "    3. Do not make up any information that is not in the resume\n",
    "    4. Be concise and professional in your responses\n",
    "    5. If the information requested is not in the context, say you don't have that information in the resume\n",
    "    \n",
    "    RESUME CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    CONVERSATION HISTORY:\n",
    "    {chat_history}\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    YOUR ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    def get_context(input_dict):\n",
    "        question = input_dict[\"question\"]\n",
    "        if isinstance(question, str):\n",
    "            return retriever.invoke(question)\n",
    "        else:\n",
    "            return [\"Error: Invalid question format\"]\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": get_context,\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 05: Create a function to filter irrelevant queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_irrelevant_query(query: str) -> bool:\n",
    "#     filter_template = \"\"\"\n",
    "#     Determine if the following question is asking about information that could be found in a resume,\n",
    "#     about someone's professional experience, education, skills, or background.\n",
    "\n",
    "#     QUESTION: {question}\n",
    "\n",
    "#     Answer only YES or NO:\n",
    "#     \"\"\"\n",
    "\n",
    "#     filter_prompt = ChatPromptTemplate.from_template(filter_template)\n",
    "#     filter_chain = filter_prompt | llm | StrOutputParser()\n",
    "\n",
    "#     result = filter_chain.invoke({\"question\": query})\n",
    "#     return \"YES\" in result.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 06: Create the main chatbot function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_chatbot(query: str, chain, memory) -> str:\n",
    "    # if not filter_irrelevant_query(query):\n",
    "    #     response = \"I'm designed to only answer questions about the resume. This doesn't appear to be related to the resume information. Is there something specific about the professional background you'd like to know?\"\n",
    "    #     memory.append(HumanMessage(content=query))\n",
    "    #     memory.append(AIMessage(content=response))\n",
    "    #     return response\n",
    "\n",
    "    chat_history = \"\"\n",
    "    for i in range(0, len(memory), 2):\n",
    "        if i + 1 < len(memory):\n",
    "            chat_history += f\"Human: {memory[i].content}\\n\"\n",
    "            chat_history += f\"AI: {memory[i+1].content}\\n\\n\"\n",
    "\n",
    "    input_data = {\"question\": query, \"chat_history\": chat_history}\n",
    "\n",
    "    try:\n",
    "        response = chain.invoke(input_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chain: {e}\")\n",
    "        response = (\n",
    "            \"I'm having trouble processing your question. Could you try rephrasing it?\"\n",
    "        )\n",
    "\n",
    "    memory.append(HumanMessage(content=query))\n",
    "    memory.append(AIMessage(content=response))\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 07 (Final Step): Bring everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_chain(vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume chatbot initialized. Ask a question about the resume or the person's professional background.\n",
      "\n",
      "Chatbot: Aryan Khurana worked on the following projects:\n",
      "\n",
      "1. **Fragments**: A universal conversion microservice that transforms any text, application, or media file into multiple formats, utilizing technologies such as Express.js, AWS (S3, DynamoDB, ECS, ECR, Cognito), Docker, GitHub Actions, Jest, and Ansible.\n",
      "\n",
      "2. **WattWise**: An IoT-powered energy monitoring system that aggregates live power data, predicts monthly costs, and flags anomalies via AI. This project won 2nd place at Hackthe6ix 2024 and utilized technologies including Express.js, MQTT, Docker, DynamoDB, Next.js, Flask, GitHub Actions, scikit-learn, and Google Gemini GenAI.\n",
      "\n",
      "\n",
      "Chatbot: I'm here to assist with questions related to the resume information provided. Please let me know if you have any specific inquiries regarding Aryan Khurana's education, work experience, projects, technical skills, or achievements.\n",
      "\n",
      "Exiting chatbot...\n"
     ]
    }
   ],
   "source": [
    "memory = []\n",
    "print(\n",
    "    \"Resume chatbot initialized. Ask a question about the resume or the person's professional background.\"\n",
    ")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Exiting chatbot...\")\n",
    "        break\n",
    "\n",
    "    response = resume_chatbot(query, chain, memory)\n",
    "    print(f\"\\nChatbot: {response}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
